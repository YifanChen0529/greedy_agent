# Code Citations

## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_agents:
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_agents:
        if agent.agent
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_agents:
        if agent.agent_id == self
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_agents:
        if agent.agent_id == self.agent_i
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
self.list_policy_new = [0 for x in range(self.n_agents)]
    self.returns = tf.placeholder(tf.float32, [None], 'returns')

    for agent in self.list_of_agents:
        if agent.agent_id == self.agent_id and not self
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads,
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name]
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new,
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id]
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            tf.reduce_sum(tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            tf.reduce_sum(tf.multiply(other_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            tf.reduce_sum(tf.multiply(other_policy_new.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
zip(agent.policy_grads, agent.policy_params):
            other_policy_params_new[var.name] = var - agent.lr_actor * grad
        
        other_policy_new = agent.policy_new(
            other_policy_params_new, agent.l_obs, agent.l_action,
            agent.agent_name)
        self.list_policy_new[agent.agent_id] = other_policy_new

        log_probs_taken = tf.log(
            tf.reduce_sum(tf.multiply(other_policy_new.probs
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_rewar
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.appen
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_i
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg ==
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.rewar
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.rewar
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)),
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)),
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step,
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step,
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_sum(list_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_sum(list_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_agent.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_sum(list_reward_loss)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
), axis=1))
        loss_term = -tf.reduce_sum(tf.multiply(log_probs_taken, self.returns))
        list_reward_loss.append(loss_term)

    if self.include_cost_in_chain_rule:
        self.reward_loss = tf.reduce_sum(list_reward_loss)
    else:
        reverse_1hot = 1 - tf.one_hot(indices=self.agent_id, depth=self.n_agents)
        if self.separate_cost_optimizer or self.reg == 'l1':
            given_each_step = tf.reduce_sum(tf.abs(
                tf.multiply(self.reward_function, reverse_1hot)), axis=1)
            total_given = tf.reduce_sum(tf.multiply(
                given_each_step, self.gamma_prod/self.gamma))
        elif self.reg == 'l2':
            total_given = tf.reduce_sum(tf.square(
                tf.multiply(self.reward_function, reverse_1hot)))
        
        if self.separate_cost_optimizer:
            self.reward_loss = tf.reduce_sum(list_reward_loss)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sg
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDes
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.G
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescent
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        rewar
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.A
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt =
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.AdamOptim
```


## License: MIT
https://github.com/011235813/lio/blob/979e29723aa9bf18fd63447bb58a2cc27ade6727/lio/alg/lio_ac.py

```
if self.optimizer == 'sgd':
        reward_opt = tf.train.GradientDescentOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.GradientDescentOptimizer(self.lr_cost)
    elif self.optimizer == 'adam':
        reward_opt = tf.train.AdamOptimizer(self.lr_reward)
        if self.separate_cost_optimizer:
            cost_opt = tf.train.AdamOptimizer(self.
```

